services:
  spark-compiler:
    build:
      context: ./java
    container_name: spark-compiler
    networks:
      - hadoop
    volumes:
      - ./output:/app/output
    depends_on:
      - hadoop-master
    entrypoint: bash -c "mvn clean package && cp /app/target/*.jar /app/output/"
  hadoop-worker1:
    image: madjidtaoualit/hadoop-cluster:latest
    platform: linux/amd64
    container_name: hadoop-worker1
    hostname: hadoop-worker1
    networks:
      - hadoop
    ports:
      - "8040:8042"
    command: ["tail", "-f", "/dev/null"]

  hadoop-worker2:
    image: madjidtaoualit/hadoop-cluster:latest
    platform: linux/amd64
    container_name: hadoop-worker2
    hostname: hadoop-worker2
    networks:
      - hadoop
    ports:
      - "8041:8042"
    command: ["tail", "-f", "/dev/null"]
  hadoop-master:
    image: madjidtaoualit/hadoop-cluster:latest
    platform: linux/amd64
    container_name: hadoop-master
    hostname: hadoop-master
    networks:
      - hadoop
    ports:
      - "9870:9870"
      - "8088:8088"
      - "7077:7077"
      - "16010:16010"
    volumes:
      - ./output:/root/output
      - ./run-spark-jobs.sh:/root/run-spark-jobs.sh
      - ./logs:/root/logs
    command: bash -c "/root/start-hadoop.sh && bash /root/run-spark-jobs.sh"
    depends_on:
      - hadoop-worker1
      - hadoop-worker2

networks:
  hadoop:
    driver: bridge